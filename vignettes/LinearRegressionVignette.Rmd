---
title: "LinearRegression package"
author: "Sunny Lee, Uthman Kareem, Tianhang Lang"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{LinearRegression package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The LinearRegression package contains the basic functions to perform linear regression and obtain different statistics from the procedure.

More specifically, it can be used to explain an $n \times 1$ vector of observations $y$ through a linear combination of $p$ explanatory variables that are stored in an $n \times p$ matrix $X$:

$$y = X^T\beta + \epsilon$$

Here $\beta$ is the $p \times 1$ coefficient vector and $\epsilon$ is the $n \times 1$ random error vector with null expectation and variance $\sigma^2$.

The package is available for download at https://github.com/AU-R-Programming/Final_group6

```{r setup}
library(LinearRegression)
```

The package contains eight functions:

* `regress(y, x, data = NULL, intercept = TRUE)` for estimating the coefficients of the linear regression model as well as their variance-covariance matrix and the residual variance;
* `results(mod, print = TRUE)` for obtaining summary results of the estimated linear regression model, including tests for significance of the coefficients, overall significance of the model and metrics to evaluate model fit;
* `coefCI(mod, alpha = 0.05, approach = "asymptotic", B = 1000)` for obtaining asymptotic or bootstrap confidence intervals for the model coefficients;
* `residualPlots(mod, which = NULL)` for obtaining diagnostic residual plots for the linear regression model;
* `MSPE(mod)` for obtaining the Mean Square Prediction Error (MSPE) of the model;
* `Ftest(mod)` for obtaining the F-test results for the overall significance of the model;
* `Rsquared(mod)` for obtaining the R-squared of the model and
* `RsquaredAdj(mod)` for obtaining the  Adjusted R-squared of the model.

This document will introduce all these functions in more detail and show examples of different ways on how to use them.

## Data: wine.csv

To explore the functionality of the LinearRegression package, we will use the `wine` dataset. The dataset contains 178 observations of 15 variables and includes information on the chemical composition of different wines.

```{r}
wine <- read.csv2("wine.csv")
str(wine)
```

## Estimating the model with `regress()`

### Simple linear regression

Let's say we want to estimate the linear regression model with `Color.intensity` as the dependent variable (`y`) and `Alcohol` as the independent variable (`x`). We can do that by running:

```{r}
model <- regress(wine$Color.intensity, wine$Alcohol)
```

Alternatively, we can specify the names of the dependent and independent variables and supply the `data.frame` containing these variables as the `data` argument. The below command is equivalent to the command above:

```{r}
model <- regress("Color.intensity", "Alcohol", data = wine)
```

The above commands assign a list of class `linreg` to `model`. This list will be used as the main argument for all the other functions in the package as we will see later. Thus, to use the other functions in the package we always need to run the `regress()` command first.

The main components of the list in `model` are the estimated coefficients of the linear regression model $\hat{\beta}$ (`coefficients`), the estimated residual variance $\hat{\sigma}^2$ (`error.variance`) and the estimated variance-covariance matrix of the estimated coefficients $Var(\hat{\beta})$ (`beta.variance`). These results can be accessed with

```{r}
model$coefficients
model$error.variance
model$beta.variance
```

The list in `model` also contains model residuals (`residuals`), fitted values $\hat{y} = X\hat{\beta}$(`fitted.values`), the dependent variable $y$ (`y`), the matrix of the independent variables $X$ (`x`), number of observations used $n$ (`n`), number of parameters estimated $p$ (`p`) and the residual degrees of freedom (`df`). These components can all be accessed similarly as above, for example,

```{r}
model$n
```

### Multiple linear regression

We can also estimate a multiple linear regression model. For example, we can regress `Color.intensity` on `Alcohol`, `Hue` and `Proline` by running either of the following commands, which are equivalent:

```{r}
model <- regress(wine$Color.intensity, wine[, c("Alcohol", "Hue", "Proline")])
model <- regress("Color.intensity", c("Alcohol", "Hue", "Proline"), data = wine)
```

We can then access the estimated coefficients and other parameters the same way as above.

```{r}
model$coefficients
```

### Regression with no intercept

By default, `regress()` estimates models with the intercept coefficient. It is possible to estimate models with no intercept by specifying argument `intercept = FALSE`.

```{r}
model2 <- regress("Color.intensity", "Alcohol", data = wine, intercept = FALSE)
model2$coefficients
model3 <- regress("Color.intensity", c("Alcohol", "Hue", "Proline"), data = wine, intercept = FALSE)
model3$coefficients
```

## Regression results with `results()`

After estimating the linear regression model with `regress()`, the main results of the fitted model can be obtained using `results()`.

```{r}
results(model)
```

As we can see, it prints out the estimated coefficients with their standard errors, t-statistic and p-value for the significance of each coefficient, as well as residual standard error, MSPE, (adjusted) R-squared and F-test results for the overall significance of the model.

The output of this function can also be assigned to an object without printing out the results and each of the statistics can be accessed individually. For example, we can extract standard errors of the coefficient estimates:

```{r}
model_results <- results(model, print = FALSE)
model_results$results[, "Std. Error"]
```

p-values for the significance of the coefficients:

```{r}
model_results$results[,  "Pr(>|t|)"]
```

Adjusted R-squared:

```{r}
model_results$AdjRsquared
```

MSPE:

```{r}
model_results$mspe
```

## Coefficient confidence intervals with `coefCI()`

Confidence intervals for the linear reggression model coefficients $\beta$ can be obtained using the `coefCI()` function.

```{r}
coefCI(model)
```

By default, the function returns asymptotic 95% confidence intervals (significance level $\alpha = 0.05$). The significance level can be changed by specifying the parameter `alpha`. For example, to obtain 90% confidence intervals ($\alpha = 0.10$):

```{r}
coefCI(model, alpha = 0.1)
```

It is also possible to obtain bootstrap confidence intervals instead of asymptotic confidence intervals by specifying `approach = "bootsrap"`. Number of bootstrap replications `B` (1000 by default) can also be adjusted:

```{r}
coefCI(model, alpha = 0.1, approach = "bootstrap", B = 2000)
```

## Residual plots with `residualPlots()`

Residual diagnostic plots for checking the assumptions of the linear regression model can be obtained using the `residualPlots()` function. Four plots are available:

1. a plot of residuals against fitted values,
2. a Normal Q-Q plot of residuals,
3. a histogram of residuals and
4. a density plot of residuals.

By default, the first three plots are provided in a single figure:

```{r, fig.height=4, fig.width=6}
residualPlots(model)
```

If we want to obtain only one of the four possible plots, we can specify the number of the desired plot with the argument `which`. For example, residuals vs. fitted plot and the density plot of residuals can be obtained separately:

```{r, fig.height=3, fig.width=4}
residualPlots(model, which = 1)
residualPlots(model, which = 4)
```

## Mean Square Prediction Error with `MSPE()`

We saw above how to extract the Mean Square Prediction Error from the output of `results()` function, however it can also be obtained directly using the `MSPE()` function.

```{r}
MSPE(model)
```

The same is true for the F-test results, R-squared and adjusted R-squared.

## F-test results with `Ftest()`

We can obtain the F-test results for the overall significance of the fitted model using the `Ftest()` function. It returns a list containing the F-statistic, degrees of freedom and the corresponding p-value.

```{r}
Ftest(model)
```

## R-squared and Adjusted R-squared with `Rsquared()` and `RsquaredAdj()`

We can obtain the R-squared of the model and the adjusted R-squared with `Rsquared()` and `RsquaredAdj()`, respectively:

```{r}
Rsquared(model)
RsquaredAdj(model)
```

@ref https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html
